# üìñ Explica√ß√£o Completa: O Que Acontece com um PDF de Artigos sobre Empresas?

## üéØ Cen√°rio

Voc√™ faz upload de um PDF chamado `artigos_empresas.pdf` que cont√©m:
- **Artigo 1**: "Apple lan√ßa novo iPhone"
- **Artigo 2**: "Microsoft anuncia parceria com OpenAI"
- **Artigo 3**: "Google desenvolve IA avan√ßada"

Voc√™ escolhe: **"Universal A2 (ETL Autom√°tico)"**

---

## üîÑ Fluxo Passo a Passo

### **FASE 1: Upload e Leitura** ‚è±Ô∏è ~2-5s

```
PDF ‚Üí Universal A2 Reader ‚Üí Default Reader ‚Üí Extra√ß√£o de Texto
```

**O que acontece:**
1. Voc√™ faz upload do `artigos_empresas.pdf`
2. `UniversalA2Reader` delega para `BasicReader` (Default)
3. `BasicReader` usa `pypdf` para extrair texto de todas as p√°ginas
4. Texto extra√≠do: `"[Artigo 1 completo]...\n\n[Artigo 2 completo]...\n\n[Artigo 3 completo]..."`

**Resultado:**
- ‚úÖ Um **Document** criado com:
  - `title`: "artigos_empresas.pdf"
  - `content`: Todo o texto extra√≠do (3 artigos juntos)
  - `meta.enable_etl = True` ‚Üê Marca para ETL posterior

---

### **FASE 2: ETL Pr√©-Chunking** ‚è±Ô∏è ~5-6s (OTIMIZADO)

```
Documento Completo ‚Üí Extra√ß√£o de Entidades ‚Üí Entity-Spans
```

**O que acontece:**
1. **Extra√ß√£o de Entidades** via spaCy NER:
   - Extrai apenas **ORG** (organiza√ß√µes) e **PERSON/PER** (pessoas)
   - Exclui LOC/GPE/MISC para performance (reduz 367 ‚Üí ~110 entidades)
   - **Deduplica** entidades duplicadas por posi√ß√£o
   - **Normaliza** PER (PT) ‚Üí PERSON (EN) para compatibilidade
   
2. **Armazenamento** em `document.meta["entity_spans"]`:
   ```python
   entity_spans = [
       {"text": "Apple", "start": 0, "end": 5, "label": "ORG"},
       {"text": "Fernando Carneiro", "start": 150, "end": 167, "label": "PERSON"},
       ...
   ]
   ```

**Otimiza√ß√µes:**
- ‚úÖ **Binary search** para filtragem O(n log n) em vez de O(n¬≤)
- ‚úÖ **Deduplica√ß√£o** evita processar entidades repetidas
- ‚úÖ **Filtro de tipos** reduz volume em 71%

**Resultado:**
- ‚úÖ ~110 entidades extra√≠das (ORG + PERSON apenas)
- ‚úÖ Armazenadas em `document.meta["entity_spans"]`
- ‚úÖ Prontas para uso no chunking entity-aware

---

### **FASE 3: Chunking Entity-Aware** ‚è±Ô∏è ~2-3s (OTIMIZADO)

```
Documento + Entity-Spans ‚Üí Section-Aware Chunker ‚Üí M√∫ltiplos Chunks
```

**O que acontece:**
1. **Section-Aware Chunker** usa `entity_spans` para:
   - **Evitar cortar entidades** no meio dos chunks
   - **Respeitar se√ß√µes** do documento
   - **Binary search** para filtrar entidades por se√ß√£o (O(log n))

2. Divide o texto em chunks respeitando:
   - Limites de se√ß√µes
   - Posi√ß√µes das entidades (n√£o corta no meio)

**Exemplo de chunks criados:**
```
Chunk 1: "Apple lan√ßa novo iPhone. A empresa americana anunciou..."
Chunk 2: "...caracter√≠sticas t√©cnicas incluem processador A17..."
Chunk 3: "Microsoft anuncia parceria estrat√©gica com OpenAI..."
Chunk 4: "...visa acelerar desenvolvimento de IA generativa..."
Chunk 5: "Google desenvolve novo modelo de IA avan√ßada..."
Chunk 6: "...chamado Gemini Pro, supera ChatGPT em v√°rios benchmarks..."
```

**Resultado:**
- ‚úÖ **6-10 chunks** criados (dependendo do tamanho do PDF)
- Cada chunk tem:
  - `text`: Texto do chunk
  - `doc_uuid`: ID do documento pai (ser√° atribu√≠do depois)

---

### **FASE 4: Embedding (Vectoriza√ß√£o)** ‚è±Ô∏è ~5-15s

```
Chunks ‚Üí Embedder ‚Üí Vetores (384/768/1536 dimens√µes)
```

**O que acontece:**
1. Verba usa o **embedder** escolhido (ex: SentenceTransformers)
2. Cada chunk √© convertido em um vetor num√©rico

**Exemplo:**
```
Chunk 1 ‚Üí [0.123, -0.456, 0.789, ..., 0.234] (384 n√∫meros)
Chunk 2 ‚Üí [0.234, -0.567, 0.890, ..., 0.345]
...
```

**Resultado:**
- ‚úÖ Cada chunk tem um **vetor de embedding**
- Vetores ser√£o usados para busca sem√¢ntica depois

---

### **FASE 5: Import no Weaviate** ‚è±Ô∏è ~2-5s

```
Chunks + Vetores ‚Üí Weaviate ‚Üí Armazenamento
```

**O que acontece:**
1. `WeaviateManager.import_document()` √© chamado
2. Documento √© inserido na collection `VERBA_Document`
3. Cada chunk √© inserido na collection do embedder (ex: `VERBA_Embedding_SentenceTransformers`)

**Estrutura no Weaviate:**
```
VERBA_Document:
  - uuid: "abc-123-def"
  - properties:
    - title: "artigos_empresas.pdf"
    - content: "[texto completo]"
    - source: "artigos_empresas.pdf"

VERBA_Embedding_SentenceTransformers:
  - uuid: "chunk-1"
  - properties:
    - text: "Apple lan√ßa novo iPhone..."
    - doc_uuid: "abc-123-def"
    - chunk_index: 0
  - vector: [0.123, -0.456, ...]
  
  - uuid: "chunk-2"
  - properties:
    - text: "...caracter√≠sticas t√©cnicas..."
    - doc_uuid: "abc-123-def"
    - chunk_index: 1
  - vector: [0.234, -0.567, ...]
  
  ... (mais chunks)
```

**Resultado:**
- ‚úÖ Documento e chunks armazenados no Weaviate
- ‚úÖ Chunks t√™m `doc_uuid` vinculado ao documento

---

### **FASE 6: Hook Detecta Import** ‚è±Ô∏è ~0.1s

```
import_document completo ‚Üí Hook detecta ‚Üí Prepara para ETL
```

**O que acontece:**
1. `patched_import_document()` verifica `document.meta.enable_etl`
2. Como est√° `True`, busca todos os `passage_uuids` do documento:
   ```python
   passages = await embedder_collection.query.fetch_objects(
       filters=Filter.by_property("doc_uuid").equal("abc-123-def"),
       limit=10000
   )
   passage_uuids = ["chunk-1", "chunk-2", "chunk-3", ...]
   ```
3. Dispara hook `'import.after'` em background (n√£o bloqueia)

**Resultado:**
- ‚úÖ Hook registrado para executar ETL
- ‚úÖ Lista de `passage_uuids` preparada

---

### **FASE 7: ETL P√≥s-Chunking Executa por Chunk** ‚è±Ô∏è ~10-30s (background)

```
Cada Chunk ‚Üí ETL A2 ‚Üí Entidades + Se√ß√µes ‚Üí Atualiza Weaviate
```

**O que acontece para CADA chunk:**

#### **7.1. Extra√ß√£o de Entidades via SpaCy**

Para o **Chunk 1**: `"Apple lan√ßa novo iPhone. A empresa americana anunciou..."`

```python
nlp = spacy.load("pt_core_news_sm")
doc = nlp("Apple lan√ßa novo iPhone. A empresa americana anunciou...")

# Entidades encontradas (apenas ORG e PERSON/PER):
entidades_encontradas = [
    {"text": "Apple", "label": "ORG"},      # Organiza√ß√£o ‚úÖ
    # "iPhone" n√£o √© extra√≠do (MISC exclu√≠do)
    # "americana" n√£o √© extra√≠do (LOC/GPE exclu√≠do)
]
```

**Nota**: ETL p√≥s-chunking extrai apenas ORG e PERSON/PER (igual ao pr√©-chunking), para consist√™ncia.

**Resultado:**
- ‚úÖ Lista de entidades por chunk (texto + label)

---

#### **7.2. Normaliza√ß√£o via Gazetteer**

Para o **Chunk 1** com entidade `"Apple"`:

```python
# Gazetteer mapeia aliases para entity_ids
gazetteer = {
    "Q312": ["Apple", "Apple Inc", "Apple Computer"],
    "Q2283": ["Microsoft", "MSFT", "Microsoft Corporation"],
    "Q95": ["Google", "Google LLC", "Alphabet"]
}

# Busca "Apple" no gazetteer
entity_ids = ["Q312"]  # Apple Inc
```

**Resultado:**
- ‚úÖ Entidades normalizadas para `entity_ids` can√¥nicos
- ‚úÖ Aliases mapeados corretamente

---

#### **7.3. Detec√ß√£o de Se√ß√µes**

Para o **Chunk 1** no contexto do documento completo:

```python
# Detecta se est√° em uma se√ß√£o espec√≠fica
# Procura por padr√µes: "## T√≠tulo", "Introdu√ß√£o", etc.

section_title = "Artigo 1: Apple lan√ßa novo iPhone"
section_first_para = "A empresa americana anunciou..."
section_entity_ids = ["Q312"]  # Entidades mencionadas nesta se√ß√£o
```

**Resultado:**
- ‚úÖ T√≠tulo de se√ß√£o identificado
- ‚úÖ Primeiro par√°grafo da se√ß√£o
- ‚úÖ Entidades da se√ß√£o

---

#### **7.4. Atualiza√ß√£o no Weaviate**

Para **cada chunk**, atualiza metadados:

```python
# Chunk 1
passage_collection.data.update(
    uuid="chunk-1",
    properties={
        "entities_local_ids": ["Q312"],           # Entidades neste chunk
        "section_title": "Artigo 1: Apple...",
        "section_first_para": "A empresa...",
        "section_entity_ids": ["Q312"],          # Entidades da se√ß√£o
        "section_scope_confidence": 0.85
    }
)

# Chunk 3 (sobre Microsoft)
passage_collection.data.update(
    uuid="chunk-3",
    properties={
        "entities_local_ids": ["Q2283"],          # Microsoft
        "section_title": "Artigo 2: Microsoft...",
        "section_first_para": "Microsoft anuncia...",
        "section_entity_ids": ["Q2283", "Q199300"]  # Microsoft + OpenAI
    }
)
```

**Resultado:**
- ‚úÖ Cada chunk tem metadados de entidades
- ‚úÖ Metadados de se√ß√£o preenchidos

---

### **FASE 8: Consolida√ß√£o no Article** ‚è±Ô∏è ~1-2s

```
Passages atualizados ‚Üí Consolida entidades ‚Üí Atualiza Article
```

**O que acontece:**
1. ETL coleta todas as `entities_local_ids` de todos os chunks
2. Cria/atualiza collection `Article` (se usar schema A2)
3. Atualiza `entities_all_ids` com todas as entidades √∫nicas

```python
# Article consolidado
article_collection.data.insert(
    properties={
        "article_id": "abc-123-def",
        "url_final": "doc://artigos_empresas.pdf",
        "title": "artigos_empresas.pdf",
        "entities_all_ids": ["Q312", "Q2283", "Q199300", "Q95"],  # Todas as empresas
        "source_domain": "local",
        "published_at": "2025-01-15"
    }
)
```

**Resultado:**
- ‚úÖ `Article` criado com todas as entidades consolidadas
- ‚úÖ Relacionamento Article ‚Üî Passages estabelecido

---

## üìä Resultado Final no Weaviate

### **Documento Original:**
```
VERBA_Document:
  - title: "artigos_empresas.pdf"
  - content: "[texto completo dos 3 artigos]"
```

### **Chunks (Passages) com ETL:**
```
Chunk 1 (Apple):
  - text: "Apple lan√ßa novo iPhone..."
  - entities_local_ids: ["Q312"]              ‚Üê Apple Inc
  - section_title: "Artigo 1: Apple..."
  - section_entity_ids: ["Q312"]

Chunk 3 (Microsoft):
  - text: "Microsoft anuncia parceria..."
  - entities_local_ids: ["Q2283"]            ‚Üê Microsoft
  - section_entity_ids: ["Q2283", "Q199300"]  ‚Üê Microsoft + OpenAI

Chunk 5 (Google):
  - text: "Google desenvolve IA..."
  - entities_local_ids: ["Q95"]              ‚Üê Google
  - section_entity_ids: ["Q95"]
```

### **Article (se usar schema A2):**
```
Article:
  - article_id: "abc-123-def"
  - title: "artigos_empresas.pdf"
  - entities_all_ids: ["Q312", "Q2283", "Q199300", "Q95"]  ‚Üê Todas as empresas
```

---

## üéØ Como Usar Depois

### **Busca por Entidade Espec√≠fica:**

Use **Entity-Aware Retriever**:
```
Query: "inova√ß√£o tecnol√≥gica"
+ Filter: entities_local_ids contains "Q312" (Apple)

Resultado:
- Retorna apenas chunks que mencionam Apple
- Evita contamina√ß√£o com Microsoft/Google
```

### **Busca por Se√ß√£o:**

```
Query: "parcerias"
+ Filter: section_entity_ids contains "Q2283" (Microsoft)

Resultado:
- Retorna apenas se√ß√£o sobre Microsoft
```

### **Busca H√≠brida:**

```
Query: "intelig√™ncia artificial"
+ Filter: entities_all_ids contains any of ["Q95", "Q2283"] (Google ou Microsoft)

Resultado:
- Chunks sobre Google ou Microsoft relacionados a IA
- Exclui Apple automaticamente
```

---

## ‚è±Ô∏è Tempo Total Estimado (OTIMIZADO)

- **Upload + Leitura**: 2-5s
- **ETL Pr√©-Chunking**: 5-6s (otimizado: 71% menos entidades)
- **Chunking Entity-Aware**: 2-3s (otimizado: binary search, 10-15x mais r√°pido)
- **Embedding**: 5-15s
- **Import Weaviate**: 2-5s
- **ETL P√≥s-Chunking (background)**: 10-30s
- **Total**: **26-64 segundos**

**Antes das otimiza√ß√µes**: 30s+ apenas no chunking  
**Depois das otimiza√ß√µes**: 2-3s no chunking  
**Ganho total**: **10-15x mais r√°pido** no chunking!

**Importante**: ETL p√≥s-chunking executa em background, ent√£o voc√™ pode continuar usando o Verba enquanto processa!

---

## üí° Pontos Importantes

### ‚úÖ **Vantagens:**
1. **Autom√°tico**: Voc√™ s√≥ faz upload e importa
2. **Por Chunk**: Entidades extra√≠das contextualmente
3. **Normalizado**: Aliases mapeados para IDs can√¥nicos
4. **Se√ß√µes**: Detec√ß√£o autom√°tica de estrutura
5. **Background**: N√£o bloqueia interface

### ‚ö†Ô∏è **Limita√ß√µes:**
1. **SpaCy**: Requer modelo instalado (`pt_core_news_sm` ou `en_core_web_sm`)
2. **Gazetteer**: Entidades precisam estar no arquivo JSON
3. **Performance**: ETL adiciona 10-30s por documento (p√≥s-chunking, em background)
4. **PDF Complexo**: Pode n√£o separar artigos automaticamente (se forem cont√≠nuos)
5. **Tipos de Entidades**: Apenas ORG e PERSON extra√≠das (LOC/GPE exclu√≠dos para performance)

### üöÄ **Otimiza√ß√µes Implementadas:**
1. **Binary Search**: Filtragem O(n¬≤) ‚Üí O(n log n) (6.7x mais r√°pido)
2. **Deduplica√ß√£o**: Remove entidades duplicadas por posi√ß√£o
3. **Filtro de Tipos**: Apenas ORG + PERSON (reduz 71% das entidades)
4. **Normaliza√ß√£o**: PER (PT) ‚Üí PERSON (EN) para compatibilidade
5. **Entity-Aware**: Chunking n√£o corta entidades no meio (qualidade mantida)

---

## üîß Ajustes Poss√≠veis

### **Se PDF tem m√∫ltiplos artigos bem separados:**

O Verba pode criar m√∫ltiplos documentos se houver quebras claras. Mas se tudo vem como um √∫nico documento:

**Op√ß√£o 1**: Use o script `pdf_to_a2_json.py` para separar manualmente
**Op√ß√£o 2**: Importe como est√° - ETL processa todos os chunks mesmo assim

### **Se alguma empresa n√£o √© detectada:**

Adicione ao `gazetteer.json`:
```json
{
  "entity_id": "Q999",
  "aliases": ["Nome da Empresa", "Nome Alternativo", "Sigla"]
}
```

---

**Agora voc√™ entende exatamente o que acontece quando faz upload de um PDF com artigos sobre empresas!** üéâ

