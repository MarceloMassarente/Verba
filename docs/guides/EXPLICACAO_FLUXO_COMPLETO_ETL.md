# üìñ Explica√ß√£o Completa: O Que Acontece com um PDF de Artigos sobre Empresas?

## üéØ Cen√°rio

Voc√™ faz upload de um PDF chamado `artigos_empresas.pdf` que cont√©m:
- **Artigo 1**: "Apple lan√ßa novo iPhone"
- **Artigo 2**: "Microsoft anuncia parceria com OpenAI"
- **Artigo 3**: "Google desenvolve IA avan√ßada"

Voc√™ escolhe: **"Universal A2 (ETL Autom√°tico)"**

---

## üîÑ Fluxo Passo a Passo

### **FASE 1: Upload e Leitura** ‚è±Ô∏è ~2-5s

```
PDF ‚Üí Universal A2 Reader ‚Üí Default Reader ‚Üí Extra√ß√£o de Texto
```

**O que acontece:**
1. Voc√™ faz upload do `artigos_empresas.pdf`
2. `UniversalA2Reader` delega para `BasicReader` (Default)
3. `BasicReader` usa `pypdf` para extrair texto de todas as p√°ginas
4. Texto extra√≠do: `"[Artigo 1 completo]...\n\n[Artigo 2 completo]...\n\n[Artigo 3 completo]..."`

**Resultado:**
- ‚úÖ Um **Document** criado com:
  - `title`: "artigos_empresas.pdf"
  - `content`: Todo o texto extra√≠do (3 artigos juntos)
  - `meta.enable_etl = True` ‚Üê Marca para ETL posterior

---

### **FASE 2: Chunking** ‚è±Ô∏è ~1-3s

```
Documento Completo ‚Üí Chunker ‚Üí M√∫ltiplos Chunks
```

**O que acontece:**
1. Verba aplica o **chunker** escolhido (ex: SentenceChunker)
2. Divide o texto em chunks de ~200-500 palavras cada

**Exemplo de chunks criados:**
```
Chunk 1: "Apple lan√ßa novo iPhone. A empresa americana anunciou..."
Chunk 2: "...caracter√≠sticas t√©cnicas incluem processador A17..."
Chunk 3: "Microsoft anuncia parceria estrat√©gica com OpenAI..."
Chunk 4: "...visa acelerar desenvolvimento de IA generativa..."
Chunk 5: "Google desenvolve novo modelo de IA avan√ßada..."
Chunk 6: "...chamado Gemini Pro, supera ChatGPT em v√°rios benchmarks..."
```

**Resultado:**
- ‚úÖ **6-10 chunks** criados (dependendo do tamanho do PDF)
- Cada chunk tem:
  - `text`: Texto do chunk
  - `doc_uuid`: ID do documento pai (ser√° atribu√≠do depois)

---

### **FASE 3: Embedding (Vectoriza√ß√£o)** ‚è±Ô∏è ~5-15s

```
Chunks ‚Üí Embedder ‚Üí Vetores (384/768/1536 dimens√µes)
```

**O que acontece:**
1. Verba usa o **embedder** escolhido (ex: SentenceTransformers)
2. Cada chunk √© convertido em um vetor num√©rico

**Exemplo:**
```
Chunk 1 ‚Üí [0.123, -0.456, 0.789, ..., 0.234] (384 n√∫meros)
Chunk 2 ‚Üí [0.234, -0.567, 0.890, ..., 0.345]
...
```

**Resultado:**
- ‚úÖ Cada chunk tem um **vetor de embedding**
- Vetores ser√£o usados para busca sem√¢ntica depois

---

### **FASE 4: Import no Weaviate** ‚è±Ô∏è ~2-5s

```
Chunks + Vetores ‚Üí Weaviate ‚Üí Armazenamento
```

**O que acontece:**
1. `WeaviateManager.import_document()` √© chamado
2. Documento √© inserido na collection `VERBA_Document`
3. Cada chunk √© inserido na collection do embedder (ex: `VERBA_Embedding_SentenceTransformers`)

**Estrutura no Weaviate:**
```
VERBA_Document:
  - uuid: "abc-123-def"
  - properties:
    - title: "artigos_empresas.pdf"
    - content: "[texto completo]"
    - source: "artigos_empresas.pdf"

VERBA_Embedding_SentenceTransformers:
  - uuid: "chunk-1"
  - properties:
    - text: "Apple lan√ßa novo iPhone..."
    - doc_uuid: "abc-123-def"
    - chunk_index: 0
  - vector: [0.123, -0.456, ...]
  
  - uuid: "chunk-2"
  - properties:
    - text: "...caracter√≠sticas t√©cnicas..."
    - doc_uuid: "abc-123-def"
    - chunk_index: 1
  - vector: [0.234, -0.567, ...]
  
  ... (mais chunks)
```

**Resultado:**
- ‚úÖ Documento e chunks armazenados no Weaviate
- ‚úÖ Chunks t√™m `doc_uuid` vinculado ao documento

---

### **FASE 5: Hook Detecta Import** ‚è±Ô∏è ~0.1s

```
import_document completo ‚Üí Hook detecta ‚Üí Prepara para ETL
```

**O que acontece:**
1. `patched_import_document()` verifica `document.meta.enable_etl`
2. Como est√° `True`, busca todos os `passage_uuids` do documento:
   ```python
   passages = await embedder_collection.query.fetch_objects(
       filters=Filter.by_property("doc_uuid").equal("abc-123-def"),
       limit=10000
   )
   passage_uuids = ["chunk-1", "chunk-2", "chunk-3", ...]
   ```
3. Dispara hook `'import.after'` em background (n√£o bloqueia)

**Resultado:**
- ‚úÖ Hook registrado para executar ETL
- ‚úÖ Lista de `passage_uuids` preparada

---

### **FASE 6: ETL Executa por Chunk** ‚è±Ô∏è ~10-30s (background)

```
Cada Chunk ‚Üí ETL A2 ‚Üí Entidades + Se√ß√µes ‚Üí Atualiza Weaviate
```

**O que acontece para CADA chunk:**

#### **6.1. Extra√ß√£o de Entidades via SpaCy**

Para o **Chunk 1**: `"Apple lan√ßa novo iPhone. A empresa americana anunciou..."`

```python
nlp = spacy.load("pt_core_news_sm")
doc = nlp("Apple lan√ßa novo iPhone. A empresa americana anunciou...")

# Entidades encontradas:
entidades_encontradas = [
    {"text": "Apple", "label": "ORG"},      # Organiza√ß√£o
    {"text": "iPhone", "label": "MISC"},    # Produto
    {"text": "americana", "label": "GPE"}   # Localiza√ß√£o
]
```

**Resultado:**
- ‚úÖ Lista de entidades por chunk (texto + label)

---

#### **6.2. Normaliza√ß√£o via Gazetteer**

Para o **Chunk 1** com entidade `"Apple"`:

```python
# Gazetteer mapeia aliases para entity_ids
gazetteer = {
    "Q312": ["Apple", "Apple Inc", "Apple Computer"],
    "Q2283": ["Microsoft", "MSFT", "Microsoft Corporation"],
    "Q95": ["Google", "Google LLC", "Alphabet"]
}

# Busca "Apple" no gazetteer
entity_ids = ["Q312"]  # Apple Inc
```

**Resultado:**
- ‚úÖ Entidades normalizadas para `entity_ids` can√¥nicos
- ‚úÖ Aliases mapeados corretamente

---

#### **6.3. Detec√ß√£o de Se√ß√µes**

Para o **Chunk 1** no contexto do documento completo:

```python
# Detecta se est√° em uma se√ß√£o espec√≠fica
# Procura por padr√µes: "## T√≠tulo", "Introdu√ß√£o", etc.

section_title = "Artigo 1: Apple lan√ßa novo iPhone"
section_first_para = "A empresa americana anunciou..."
section_entity_ids = ["Q312"]  # Entidades mencionadas nesta se√ß√£o
```

**Resultado:**
- ‚úÖ T√≠tulo de se√ß√£o identificado
- ‚úÖ Primeiro par√°grafo da se√ß√£o
- ‚úÖ Entidades da se√ß√£o

---

#### **6.4. Atualiza√ß√£o no Weaviate**

Para **cada chunk**, atualiza metadados:

```python
# Chunk 1
passage_collection.data.update(
    uuid="chunk-1",
    properties={
        "entities_local_ids": ["Q312"],           # Entidades neste chunk
        "section_title": "Artigo 1: Apple...",
        "section_first_para": "A empresa...",
        "section_entity_ids": ["Q312"],          # Entidades da se√ß√£o
        "section_scope_confidence": 0.85
    }
)

# Chunk 3 (sobre Microsoft)
passage_collection.data.update(
    uuid="chunk-3",
    properties={
        "entities_local_ids": ["Q2283"],          # Microsoft
        "section_title": "Artigo 2: Microsoft...",
        "section_first_para": "Microsoft anuncia...",
        "section_entity_ids": ["Q2283", "Q199300"]  # Microsoft + OpenAI
    }
)
```

**Resultado:**
- ‚úÖ Cada chunk tem metadados de entidades
- ‚úÖ Metadados de se√ß√£o preenchidos

---

### **FASE 7: Consolida√ß√£o no Article** ‚è±Ô∏è ~1-2s

```
Passages atualizados ‚Üí Consolida entidades ‚Üí Atualiza Article
```

**O que acontece:**
1. ETL coleta todas as `entities_local_ids` de todos os chunks
2. Cria/atualiza collection `Article` (se usar schema A2)
3. Atualiza `entities_all_ids` com todas as entidades √∫nicas

```python
# Article consolidado
article_collection.data.insert(
    properties={
        "article_id": "abc-123-def",
        "url_final": "doc://artigos_empresas.pdf",
        "title": "artigos_empresas.pdf",
        "entities_all_ids": ["Q312", "Q2283", "Q199300", "Q95"],  # Todas as empresas
        "source_domain": "local",
        "published_at": "2025-01-15"
    }
)
```

**Resultado:**
- ‚úÖ `Article` criado com todas as entidades consolidadas
- ‚úÖ Relacionamento Article ‚Üî Passages estabelecido

---

## üìä Resultado Final no Weaviate

### **Documento Original:**
```
VERBA_Document:
  - title: "artigos_empresas.pdf"
  - content: "[texto completo dos 3 artigos]"
```

### **Chunks (Passages) com ETL:**
```
Chunk 1 (Apple):
  - text: "Apple lan√ßa novo iPhone..."
  - entities_local_ids: ["Q312"]              ‚Üê Apple Inc
  - section_title: "Artigo 1: Apple..."
  - section_entity_ids: ["Q312"]

Chunk 3 (Microsoft):
  - text: "Microsoft anuncia parceria..."
  - entities_local_ids: ["Q2283"]            ‚Üê Microsoft
  - section_entity_ids: ["Q2283", "Q199300"]  ‚Üê Microsoft + OpenAI

Chunk 5 (Google):
  - text: "Google desenvolve IA..."
  - entities_local_ids: ["Q95"]              ‚Üê Google
  - section_entity_ids: ["Q95"]
```

### **Article (se usar schema A2):**
```
Article:
  - article_id: "abc-123-def"
  - title: "artigos_empresas.pdf"
  - entities_all_ids: ["Q312", "Q2283", "Q199300", "Q95"]  ‚Üê Todas as empresas
```

---

## üéØ Como Usar Depois

### **Busca por Entidade Espec√≠fica:**

Use **Entity-Aware Retriever**:
```
Query: "inova√ß√£o tecnol√≥gica"
+ Filter: entities_local_ids contains "Q312" (Apple)

Resultado:
- Retorna apenas chunks que mencionam Apple
- Evita contamina√ß√£o com Microsoft/Google
```

### **Busca por Se√ß√£o:**

```
Query: "parcerias"
+ Filter: section_entity_ids contains "Q2283" (Microsoft)

Resultado:
- Retorna apenas se√ß√£o sobre Microsoft
```

### **Busca H√≠brida:**

```
Query: "intelig√™ncia artificial"
+ Filter: entities_all_ids contains any of ["Q95", "Q2283"] (Google ou Microsoft)

Resultado:
- Chunks sobre Google ou Microsoft relacionados a IA
- Exclui Apple automaticamente
```

---

## ‚è±Ô∏è Tempo Total Estimado

- **Upload + Leitura**: 2-5s
- **Chunking**: 1-3s
- **Embedding**: 5-15s
- **Import Weaviate**: 2-5s
- **ETL (background)**: 10-30s
- **Total**: **20-58 segundos**

**Importante**: ETL executa em background, ent√£o voc√™ pode continuar usando o Verba enquanto processa!

---

## üí° Pontos Importantes

### ‚úÖ **Vantagens:**
1. **Autom√°tico**: Voc√™ s√≥ faz upload e importa
2. **Por Chunk**: Entidades extra√≠das contextualmente
3. **Normalizado**: Aliases mapeados para IDs can√¥nicos
4. **Se√ß√µes**: Detec√ß√£o autom√°tica de estrutura
5. **Background**: N√£o bloqueia interface

### ‚ö†Ô∏è **Limita√ß√µes:**
1. **SpaCy**: Requer modelo instalado (`pt_core_news_sm`)
2. **Gazetteer**: Entidades precisam estar no arquivo JSON
3. **Performance**: ETL adiciona 10-30s por documento
4. **PDF Complexo**: Pode n√£o separar artigos automaticamente (se forem cont√≠nuos)

---

## üîß Ajustes Poss√≠veis

### **Se PDF tem m√∫ltiplos artigos bem separados:**

O Verba pode criar m√∫ltiplos documentos se houver quebras claras. Mas se tudo vem como um √∫nico documento:

**Op√ß√£o 1**: Use o script `pdf_to_a2_json.py` para separar manualmente
**Op√ß√£o 2**: Importe como est√° - ETL processa todos os chunks mesmo assim

### **Se alguma empresa n√£o √© detectada:**

Adicione ao `gazetteer.json`:
```json
{
  "entity_id": "Q999",
  "aliases": ["Nome da Empresa", "Nome Alternativo", "Sigla"]
}
```

---

**Agora voc√™ entende exatamente o que acontece quando faz upload de um PDF com artigos sobre empresas!** üéâ

