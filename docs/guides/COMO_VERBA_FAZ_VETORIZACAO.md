# Como o Verba Faz a VetorizaÃ§Ã£o

## âœ… Resposta Direta

**SIM, o Verba faz a vetorizaÃ§Ã£o!** O Verba usa **embedders** (componentes de vetorizaÃ§Ã£o) para gerar os vetores dos documentos e queries, e entÃ£o envia esses vetores jÃ¡ prontos para o Weaviate.

---

## ğŸ” Como Funciona

### 1. **Verba Gera os Vetores (BYOV - Bring Your Own Vectors)**

O Verba **nÃ£o depende** do Weaviate para gerar vetores. Ele usa seus prÃ³prios **embedders**:

```python
# goldenverba/components/managers.py

embedders = [
    OpenAIEmbedder(),           # Usa API OpenAI
    SentenceTransformersEmbedder(),  # Usa HuggingFace (local)
    CohereEmbedder(),           # Usa API Cohere
    VoyageAIEmbedder(),         # Usa API VoyageAI
    UpstageEmbedder(),          # Usa API Upstage
    OllamaEmbedder(),           # Usa Ollama (local)
    WeaviateEmbedder(),         # Usa Weaviate (se configurado)
]
```

### 2. **Processo de VetorizaÃ§Ã£o**

#### **Durante o Import:**

```python
# 1. Documento Ã© lido e chunked
documents = await reader_manager.load(...)
documents = await chunker_manager.chunk(..., documents, embedder)

# 2. EmbeddingManager vetoriza os chunks
documents = await embedding_manager.vectorize(embedder, fileConfig, documents, logger)

# 3. Cada chunk recebe seu vetor
for chunk in document.chunks:
    chunk.vector = [0.123, -0.456, 0.789, ...]  # Vetor gerado pelo embedder

# 4. Vetores sÃ£o enviados para Weaviate
await weaviate_manager.import_document(client, document, embedder)
```

#### **Durante a Query:**

```python
# 1. Query Ã© vetorizada com o mesmo embedder
vector = await embedding_manager.vectorize_query(embedder, query, rag_config)

# 2. Busca no Weaviate usando o vetor
results = await weaviate_manager.hybrid_chunks(
    query=query,
    vector=vector,  # Vetor jÃ¡ gerado pelo Verba
    ...
)
```

---

## ğŸ“Š Exemplo: OpenAIEmbedder

```python
# goldenverba/components/embedding/OpenAIEmbedder.py

async def vectorize(self, config: dict, content: List[str]) -> List[List[float]]:
    """Vectorize usando API OpenAI"""
    model = config.get("Model").value  # ex: "text-embedding-3-small"
    api_key = get_environment(config, "API Key", "OPENAI_API_KEY", ...)
    
    # Chama API OpenAI para gerar vetores
    payload = {"input": content, "model": model}
    async with aiohttp.ClientSession() as session:
        async with session.post(
            f"{base_url}/embeddings",
            headers={"Authorization": f"Bearer {api_key}"},
            data=json.dumps(payload),
        ) as response:
            data = await response.json()
            embeddings = [item["embedding"] for item in data["data"]]
            return embeddings  # Retorna vetores prontos
```

**O que acontece:**
1. âœ… Verba chama API OpenAI
2. âœ… OpenAI retorna vetores
3. âœ… Verba armazena vetores nos chunks
4. âœ… Verba envia chunks + vetores para Weaviate

---

## ğŸ¯ Por Que BYOV (Bring Your Own Vectors)?

### **Vantagens:**

1. **Flexibilidade**: Escolha qualquer embedder (OpenAI, Cohere, local, etc.)
2. **Performance**: VetorizaÃ§Ã£o pode ser feita em batch antes de enviar ao Weaviate
3. **Custo**: Controle sobre qual serviÃ§o usar (pode ser local com SentenceTransformers)
4. **Cache**: Verba pode cachear embeddings (implementado em `verba_extensions/utils/embeddings_cache.py`)
5. **IndependÃªncia**: NÃ£o depende de mÃ³dulos do Weaviate

### **Weaviate em Modo BYOV:**

```dockerfile
# Dockerfile.weaviate
ENV ENABLE_MODULES=""  # Sem mÃ³dulos de vetorizaÃ§Ã£o
ENV DEFAULT_VECTORIZER_MODULE="none"  # BYOV mode
```

**O Weaviate apenas:**
- âœ… Armazena os vetores (jÃ¡ gerados pelo Verba)
- âœ… Faz busca vetorial (similarity search)
- âœ… Faz BM25 (keyword search nativo)
- âœ… Faz hybrid search (combina BM25 + vector)

**O Weaviate NÃƒO:**
- âŒ Gera vetores (isso Ã© feito pelo Verba)
- âŒ Precisa de mÃ³dulos de vetorizaÃ§Ã£o (text2vec-openai, etc.)

---

## ğŸ“‹ Embedders DisponÃ­veis

### **Cloud (APIs Externas)**
- **OpenAIEmbedder**: `text-embedding-3-small`, `text-embedding-3-large`, etc.
- **CohereEmbedder**: Modelos Cohere
- **VoyageAIEmbedder**: Modelos VoyageAI
- **UpstageEmbedder**: Modelos Upstage

### **Local**
- **SentenceTransformersEmbedder**: HuggingFace (ex: `all-MiniLM-L6-v2`)
- **OllamaEmbedder**: Modelos Ollama locais

### **Weaviate (Opcional)**
- **WeaviateEmbedder**: Usa mÃ³dulos do Weaviate (se configurado)

---

## ğŸ”„ Fluxo Completo

### **Import de Documento:**

```
1. Documento â†’ Reader â†’ Texto
2. Texto â†’ Chunker â†’ Chunks
3. Chunks â†’ Embedder â†’ Vetores  â† VERBA FAZ AQUI
4. Chunks + Vetores â†’ Weaviate â†’ Armazenado
```

### **Query:**

```
1. Query â†’ Embedder â†’ Vetor  â† VERBA FAZ AQUI
2. Vetor + Query â†’ Weaviate â†’ Busca hÃ­brida (BM25 + Vector)
3. Weaviate â†’ Retorna chunks relevantes
4. Chunks â†’ Generator â†’ Resposta final
```

---

## ğŸ’¡ Por Que Isso Importa?

### **Para o Dockerfile.weaviate:**

Como o Verba faz a vetorizaÃ§Ã£o, o Weaviate pode rodar em **modo BYOV** (sem mÃ³dulos):

```dockerfile
ENV ENABLE_MODULES=""  # Sem mÃ³dulos necessÃ¡rios
ENV DEFAULT_VECTORIZER_MODULE="none"  # BYOV
```

**BenefÃ­cios:**
- âœ… Weaviate mais leve (sem mÃ³dulos)
- âœ… Menos dependÃªncias
- âœ… Mais rÃ¡pido (sem overhead de mÃ³dulos)
- âœ… Mais flexÃ­vel (mude embedder sem reconfigurar Weaviate)

---

## ğŸ¯ Resumo

| Componente | Responsabilidade |
|------------|------------------|
| **Verba (Embedders)** | âœ… Gera vetores (BYOV) |
| **Weaviate** | âœ… Armazena vetores<br>âœ… Busca vetorial<br>âœ… BM25 (keyword)<br>âœ… Hybrid search |

**O Verba Ã© responsÃ¡vel pela vetorizaÃ§Ã£o, o Weaviate apenas armazena e busca!**

---

## ğŸ“š ReferÃªncias

- [Embedder: Import vs Query](./EMBEDDER_IMPORT_VS_QUERY.md)
- [Dockerfile.weaviate Railway Guide](./DOCKERFILE_WEAVIATE_RAILWAY.md)
- [SentenceTransformers Guide](./GUIA_SENTENCE_TRANSFORMERS.md)

---

**Ãšltima atualizaÃ§Ã£o:** Novembro 2025

